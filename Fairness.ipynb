{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28097133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from aif360.datasets import AdultDataset, BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e9c12",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# privileged and unprivileged groups\n",
    "privileged_groups = [{'sex': 1, 'age_binary': 1}] # old white males\n",
    "unprivileged_groups = [{'sex': 0, 'age_binary': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f51bb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    }
   ],
   "source": [
    "#TODO : add brief explanation in a markdown cell before this code block\n",
    "\n",
    "# custom processing for the dataset\n",
    "def custom_preprocessing(df):\n",
    "    median_age = df['age'].median()\n",
    "    df['age_binary'] = (df['age'] > median_age).astype(float)\n",
    "    df.drop(columns=['age'], inplace=True)\n",
    "    df['race'] = (df['race'] == 'White').astype(float)\n",
    "    df['sex'] = (df['sex'] == 'Male').astype(float)\n",
    "    return df\n",
    "\n",
    "dataset = AdultDataset(custom_preprocessing=custom_preprocessing,\n",
    "                              protected_attribute_names=['age_binary', 'sex'],\n",
    "                              privileged_classes=[np.array([1.0]), np.array([1.0])] ) # old white males\n",
    "\n",
    "# Get the dataset and split into train and test\n",
    "np.random.seed(1)\n",
    "dataset_orig_train, dataset_orig_vt = dataset.split([0.7], shuffle=True)\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e938c5fe",
   "metadata": {},
   "source": [
    "#### Clean up training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f652e55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31655, 98)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age_binary', 'sex']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['education-num', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'age_binary', 'workclass=Federal-gov', 'workclass=Local-gov', 'workclass=Private', 'workclass=Self-emp-inc', 'workclass=Self-emp-not-inc', 'workclass=State-gov', 'workclass=Without-pay', 'education=10th', 'education=11th', 'education=12th', 'education=1st-4th', 'education=5th-6th', 'education=7th-8th', 'education=9th', 'education=Assoc-acdm', 'education=Assoc-voc', 'education=Bachelors', 'education=Doctorate', 'education=HS-grad', 'education=Masters', 'education=Preschool', 'education=Prof-school', 'education=Some-college', 'marital-status=Divorced', 'marital-status=Married-AF-spouse', 'marital-status=Married-civ-spouse', 'marital-status=Married-spouse-absent', 'marital-status=Never-married', 'marital-status=Separated', 'marital-status=Widowed', 'occupation=Adm-clerical', 'occupation=Armed-Forces', 'occupation=Craft-repair', 'occupation=Exec-managerial', 'occupation=Farming-fishing', 'occupation=Handlers-cleaners', 'occupation=Machine-op-inspct', 'occupation=Other-service', 'occupation=Priv-house-serv', 'occupation=Prof-specialty', 'occupation=Protective-serv', 'occupation=Sales', 'occupation=Tech-support', 'occupation=Transport-moving', 'relationship=Husband', 'relationship=Not-in-family', 'relationship=Other-relative', 'relationship=Own-child', 'relationship=Unmarried', 'relationship=Wife', 'native-country=Cambodia', 'native-country=Canada', 'native-country=China', 'native-country=Columbia', 'native-country=Cuba', 'native-country=Dominican-Republic', 'native-country=Ecuador', 'native-country=El-Salvador', 'native-country=England', 'native-country=France', 'native-country=Germany', 'native-country=Greece', 'native-country=Guatemala', 'native-country=Haiti', 'native-country=Holand-Netherlands', 'native-country=Honduras', 'native-country=Hong', 'native-country=Hungary', 'native-country=India', 'native-country=Iran', 'native-country=Ireland', 'native-country=Italy', 'native-country=Jamaica', 'native-country=Japan', 'native-country=Laos', 'native-country=Mexico', 'native-country=Nicaragua', 'native-country=Outlying-US(Guam-USVI-etc)', 'native-country=Peru', 'native-country=Philippines', 'native-country=Poland', 'native-country=Portugal', 'native-country=Puerto-Rico', 'native-country=Scotland', 'native-country=South', 'native-country=Taiwan', 'native-country=Thailand', 'native-country=Trinadad&Tobago', 'native-country=United-States', 'native-country=Vietnam', 'native-country=Yugoslavia']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "972e1893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.363363\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6153e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea094a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 50 first instance weights originally:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "The 50 first instance weights after reweighing:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.33826683, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.81264928, 1.        , 1.        , 1.33826683,\n",
       "       1.33826683, 1.        , 1.        , 1.33826683, 1.        ,\n",
       "       1.33826683, 1.        , 1.        , 1.        , 1.33826683,\n",
       "       0.56633898, 1.33826683, 1.        , 0.81264928, 1.        ,\n",
       "       1.        , 0.81264928, 1.        , 1.33826683, 1.33826683,\n",
       "       0.81264928, 0.81264928, 0.56633898, 1.33826683, 0.81264928,\n",
       "       1.33826683, 0.56633898, 0.81264928, 1.33826683, 0.56633898,\n",
       "       3.31574203, 0.81264928, 1.        , 3.31574203, 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.56633898, 1.        ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The 50 first instance weights originally:')\n",
    "print(dataset.instance_weights[:50])\n",
    "\n",
    "print('The 50 first instance weights after reweighing:')\n",
    "dataset_transf_train.instance_weights[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "709c09cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                         unprivileged_groups=unprivileged_groups,\n",
    "                                         privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55c03e",
   "metadata": {},
   "source": [
    "## Train classifier on original/weighted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13a3a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract features and labels ---\n",
    "def extract_xy(dataset):\n",
    "    return dataset.features, dataset.labels.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = extract_xy(dataset_orig_train)\n",
    "X_valid, y_valid = extract_xy(dataset_orig_valid)\n",
    "X_test,  y_test  = extract_xy(dataset_orig_test)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "X_train_transf, y_train_transf = extract_xy(dataset_transf_train)\n",
    "w_train_transf = dataset_transf_train.instance_weights\n",
    "X_train_transf_scaled = scaler.fit_transform(X_train_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d568251",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier_origin = LogisticRegression(max_iter=1000)\n",
    "lr_classifier_origin.fit(X_train_scaled, y_train)\n",
    "y_test_orig_pred = lr_classifier_origin.predict(X_test_scaled)\n",
    "\n",
    "lr_classifier_transf = LogisticRegression(max_iter=1000)\n",
    "lr_classifier_transf.fit(X_train_transf_scaled, y_train_transf,\n",
    "                         sample_weight=w_train_transf)\n",
    "y_test_transf_pred = lr_classifier_transf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c323f7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION\n",
      "MODEL (No Reweighing)\n",
      "Accuracy: 0.8536\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Low Income       0.88      0.93      0.90      5079\n",
      " High Income       0.75      0.63      0.68      1705\n",
      "\n",
      "    accuracy                           0.85      6784\n",
      "   macro avg       0.81      0.78      0.79      6784\n",
      "weighted avg       0.85      0.85      0.85      6784\n",
      "\n",
      "Statistical Parity Difference: -0.3649\n",
      "====================\n",
      "MODEL (With Reweighing)\n",
      "Accuracy: 0.8325\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Low Income       0.85      0.95      0.89      5079\n",
      " High Income       0.76      0.49      0.60      1705\n",
      "\n",
      "    accuracy                           0.83      6784\n",
      "   macro avg       0.80      0.72      0.75      6784\n",
      "weighted avg       0.82      0.83      0.82      6784\n",
      "\n",
      "Statistical Parity Difference: -0.1441\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "print(\"MODEL EVALUATION\")\n",
    "\n",
    "print(\"MODEL (No Reweighing)\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_orig_pred):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_orig_pred, target_names=['Low Income', 'High Income']))\n",
    "\n",
    "# Fairness metric for original model\n",
    "dataset_orig_test_pred = dataset_orig_test.copy()\n",
    "dataset_orig_test_pred.labels = y_test_orig_pred\n",
    "metric_orig_test = ClassificationMetric(dataset_orig_test, dataset_orig_test_pred,\n",
    "                                        unprivileged_groups=unprivileged_groups,\n",
    "                                        privileged_groups=privileged_groups)\n",
    "print(f\"Statistical Parity Difference: {metric_orig_test.statistical_parity_difference():.4f}\")\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(\"MODEL (With Reweighing)\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_transf_pred):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_transf_pred, target_names=['Low Income', 'High Income']))\n",
    "\n",
    "# Fairness metric for transformed model\n",
    "dataset_transf_test_pred = dataset_orig_test.copy()\n",
    "dataset_transf_test_pred.labels = y_test_transf_pred\n",
    "metric_transf_test = ClassificationMetric(dataset_orig_test, dataset_transf_test_pred,\n",
    "                                          unprivileged_groups=unprivileged_groups,\n",
    "                                          privileged_groups=privileged_groups)\n",
    "print(f\"Statistical Parity Difference: {metric_transf_test.statistical_parity_difference():.4f}\")\n",
    "#TODO : SPD value (with reweighing) non-zero, why ?\n",
    "#TODO : add other fairness metrics : balanced accuracy, disparate impact, average odds difference, (others, maybe?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0301f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARISON\n",
      "Original Model Accuracy:    0.8536\n",
      "Transformed Model Accuracy: 0.8325\n",
      "Accuracy Difference:        -0.0211\n",
      "Original Model SPD:         -0.3649\n",
      "Transformed Model SPD:      -0.1441\n",
      "SPD Improvement:            -0.2208\n",
      "Note: Statistical Parity Difference closer to 0 indicates better fairness\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPARISON\")\n",
    "print(f\"Original Model Accuracy:    {accuracy_score(y_test, y_test_orig_pred):.4f}\")\n",
    "print(f\"Transformed Model Accuracy: {accuracy_score(y_test, y_test_transf_pred):.4f}\")\n",
    "print(f\"Accuracy Difference:        {accuracy_score(y_test, y_test_transf_pred) - accuracy_score(y_test, y_test_orig_pred):.4f}\")\n",
    "print(f\"Original Model SPD:         {metric_orig_test.statistical_parity_difference():.4f}\")\n",
    "print(f\"Transformed Model SPD:      {metric_transf_test.statistical_parity_difference():.4f}\")\n",
    "print(f\"SPD Improvement:            {abs(metric_transf_test.statistical_parity_difference()) - abs(metric_orig_test.statistical_parity_difference()):.4f}\")\n",
    "print(\"Note: Statistical Parity Difference closer to 0 indicates better fairness\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
